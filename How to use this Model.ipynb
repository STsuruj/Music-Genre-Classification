{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "pqc6ovQEVhKX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SR = 22050           # sampling rate\n",
        "DURATION = 30        # seconds to use per clip\n",
        "N_MFCC = 40\n",
        "HOP_LENGTH = 512\n",
        "MAX_FRAMES = int(np.ceil(SR * DURATION / HOP_LENGTH))  # around 1293 for 30s"
      ],
      "metadata": {
        "id": "kxvECd7zWs1A"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvLSTMGenre(nn.Module):\n",
        "    def __init__(self, n_mfcc=N_MFCC, hidden_size=128, n_classes=10, n_lstm_layers=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "        # CNN extractor on frequency axis per time-slice using Conv1d across time\n",
        "        # Input: (batch, n_mfcc, time)\n",
        "        self.conv1 = nn.Conv1d(in_channels=n_mfcc, out_channels=128, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2)  # halves time dimension\n",
        "\n",
        "        self.conv2 = nn.Conv1d(128, 256, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=2)  # halves again\n",
        "\n",
        "        # after conv/pool, time dimension reduced: MAX_FRAMES / 4 approx\n",
        "        # LSTM expects (batch, seq_len, feat); we'll transpose appropriately\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(input_size=256, hidden_size=hidden_size, num_layers=n_lstm_layers,\n",
        "                            batch_first=True, bidirectional=True, dropout=dropout if n_lstm_layers>1 else 0.0)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc1 = nn.Linear(hidden_size*2, 128)  # *2 for bidirectional\n",
        "        self.fc2 = nn.Linear(128, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, n_mfcc, time)\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool1(x)   # (batch, 128, time/2)\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool2(x)   # (batch, 256, time/4)\n",
        "\n",
        "        # transpose to (batch, time_seq, feat)\n",
        "        x = x.permute(0, 2, 1)  # (batch, seq_len, 256)\n",
        "\n",
        "        # LSTM\n",
        "        out, (hn, cn) = self.lstm(x)  # out: (batch, seq_len, hidden*2)\n",
        "        # use mean pooling over time\n",
        "        out = torch.mean(out, dim=1)  # (batch, hidden*2)\n",
        "\n",
        "        out = self.dropout(F.relu(self.fc1(out)))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "aJRJt4CVVXA4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Load checkpoint\n",
        "checkpoint = torch.load(\"/content/Music_genre_model.pth\", map_location='cpu', weights_only=False)\n",
        "\n",
        "# Recreate model architecture\n",
        "loaded_model = ConvLSTMGenre(\n",
        "    n_mfcc=checkpoint['n_mfcc'],\n",
        "    hidden_size=checkpoint['hidden_size'],\n",
        "    n_classes=checkpoint['n_classes'],\n",
        "    n_lstm_layers=checkpoint['n_lstm_layers']\n",
        ")\n",
        "\n",
        "# Load trained weights\n",
        "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "loaded_model.eval()  # evaluation mode\n",
        "\n",
        "# Restore label classes\n",
        "classes = checkpoint['label_classes']\n"
      ],
      "metadata": {
        "id": "A1gDQnntTylv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_extract_mfcc(path, augment=False):\n",
        "    # load full or truncated to DURATION\n",
        "    try:\n",
        "        y, sr = librosa.load(path, sr=SR, duration=DURATION)\n",
        "    except Exception as e:\n",
        "        raise e\n",
        "    # if too short, pad\n",
        "    if len(y) < SR * DURATION:\n",
        "        pad_len = SR * DURATION - len(y)\n",
        "        y = np.pad(y, (0, int(pad_len)), mode='constant')\n",
        "    # augmentation\n",
        "    if augment:\n",
        "        y = augment_audio(y, sr)\n",
        "        # after augment we may have different length -> ensure trim/pad\n",
        "        if len(y) < SR * DURATION:\n",
        "            y = np.pad(y, (0, int(SR * DURATION - len(y))), mode='constant')\n",
        "        if len(y) > SR * DURATION:\n",
        "            y = y[:SR * DURATION]\n",
        "    # compute MFCC (shape: n_mfcc x t)\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC, hop_length=HOP_LENGTH)\n",
        "    # transpose -> (time_steps, n_mfcc)\n",
        "    mfcc = mfcc.T.astype(np.float32)\n",
        "    # pad or truncate to MAX_FRAMES\n",
        "    if mfcc.shape[0] < MAX_FRAMES:\n",
        "        pad_width = MAX_FRAMES - mfcc.shape[0]\n",
        "        mfcc = np.pad(mfcc, ((0, pad_width), (0,0)), mode='constant')\n",
        "    elif mfcc.shape[0] > MAX_FRAMES:\n",
        "        mfcc = mfcc[:MAX_FRAMES, :]\n",
        "    return mfcc  # shape: (MAX_FRAMES, N_MFCC)\n"
      ],
      "metadata": {
        "id": "6l0_IBzDWJhg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_genre(file_path):\n",
        "    mfcc = load_and_extract_mfcc(file_path).T  # MFCC extraction\n",
        "    X = torch.from_numpy(mfcc).unsqueeze(0)    # add batch dim\n",
        "    loaded_model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = loaded_model(X)\n",
        "        pred = torch.argmax(output, dim=1).item()\n",
        "    return classes[pred]\n"
      ],
      "metadata": {
        "id": "WW-wifmKUnIH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "song_path = \"/content/jazz.00001.wav\"\n",
        "print(\"Predicted genre:\", predict_genre(song_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZuTzB2CX3zA",
        "outputId": "26e504e8-d2d1-4a5a-ea12-d4bc55a0c07c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted genre: jazz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "song_path = \"/content/pop.00003.wav\"\n",
        "print(\"Predicted genre:\", predict_genre(song_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-Gb7X95XCBA",
        "outputId": "b7594db9-9d75-49e7-cac4-0daee38b829f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted genre: pop\n"
          ]
        }
      ]
    }
  ]
}